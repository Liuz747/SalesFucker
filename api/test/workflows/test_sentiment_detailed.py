#!/usr/bin/env python3
"""
å¢å¼ºç‰ˆ Sentiment Chat Workflow æµ‹è¯•è„šæœ¬ - æ˜¾ç¤ºå®Œæ•´è°ƒè¯•ä¿¡æ¯

æ–°å¢åŠŸèƒ½:
â€¢ æ˜¾ç¤ºæ¯ä¸ª agent æ¥æ”¶åˆ°çš„å®Œæ•´æç¤ºè¯
â€¢ æ˜¾ç¤º LLM çš„åŸå§‹å“åº”å’Œè§£æç»“æœ
â€¢ æ˜¾ç¤ºè¯¦ç»†çš„ä¸­é—´å¤„ç†æ­¥éª¤
â€¢ æ”¯æŒè‡ªå®šä¹‰æµ‹è¯•ç”¨ä¾‹å’Œæ‰¹é‡æµ‹è¯•

ä½¿ç”¨æ–¹æ³•:
    # åŸºç¡€æµ‹è¯•
    cd api && python test/workflows/test_sentiment_detailed.py --real-llm --message "ä½ å¥½"

    # å¤šæ¨¡æ€æ ¼å¼æµ‹è¯• (æ‚¨çš„åŸå§‹è¯·æ±‚æ ¼å¼)
    cd api && python test/workflows/test_sentiment_detailed.py --real-llm --multimodal

    # æ‰¹é‡æµ‹è¯•å¤šä¸ªæ¡ˆä¾‹ (åŒ…å«å¤šæ¨¡æ€)
    cd api && python test/workflows/test_sentiment_detailed.py --real-llm --batch

    # æ˜¾ç¤ºæè¯¦ç»†çš„è°ƒè¯•ä¿¡æ¯
    cd api && python test/workflows/test_sentiment_detailed.py --real-llm --message "ä½ å¥½" --verbose
"""

import asyncio
import sys
import argparse
import os
import json
from pathlib import Path
from uuid import uuid4
from typing import Dict, Any, List

SCRIPT_DIR = Path(__file__).resolve().parent  # api/test/workflows/
API_DIR = SCRIPT_DIR.parent.parent  # api/
if str(API_DIR) not in sys.path:
    sys.path.insert(0, str(API_DIR))


def _load_env_file(env_path: Path) -> bool:
    """Load key=value pairs into environment without overriding existing values."""
    if not env_path.exists():
        return False

    for raw_line in env_path.read_text(encoding="utf-8").splitlines():
        line = raw_line.strip()
        if not line or line.startswith("#"):
            continue
        if "=" not in line:
            continue
        key, value = line.split("=", 1)
        key = key.strip()
        if not key or key.startswith("#") or key in os.environ:
            continue

        stripped = value.strip().strip('"').strip("'")
        os.environ[key] = stripped

    return True


def load_local_env():
    """Best-effort load of workspace .env files before importing app modules."""
    candidates = [
        API_DIR / ".env.local",
        API_DIR / ".env",
        API_DIR.parent / ".env.local",
        API_DIR.parent / ".env",
    ]
    for env_file in candidates:
        _load_env_file(env_file)


# Load environment configuration once before pulling in app modules
load_local_env()

# è®¾ç½®å¿…è¦çš„ç¯å¢ƒå˜é‡ (åœ¨å¯¼å…¥ä»»ä½•æ¨¡å—ä¹‹å‰)
os.environ.setdefault('CALLBACK_URL', 'http://localhost:8000')
os.environ.setdefault('APP_KEY', 'test_key')


class DetailedLLMInspector:
    """ç”¨äºæ•è·å’Œæ˜¾ç¤º LLM è°ƒç”¨è¯¦æƒ…çš„å·¥å…·ç±»"""

    def __init__(self, verbose=False):
        self.verbose = verbose
        self.call_history = []

    def log_llm_call(self, agent_name: str, prompt: str, response: Any, model_info: Dict):
        """è®°å½• LLM è°ƒç”¨ä¿¡æ¯"""
        call_info = {
            "agent_name": agent_name,
            "model_info": model_info,
            "prompt": prompt,
            "response": response,
            "timestamp": asyncio.get_event_loop().time()
        }
        self.call_history.append(call_info)

    def display_llm_call(self, agent_name: str, prompt: str, response: Any, model_info: Dict):
        """æ˜¾ç¤ºå•æ¬¡ LLM è°ƒç”¨çš„è¯¦ç»†ä¿¡æ¯"""
        print(f"\n{'ğŸ” LLM è°ƒç”¨è¯¦æƒ…' : ^50}")
        print("â”€" * 50)
        print(f"ğŸ¤– Agent: {agent_name}")
        print(f"ğŸ§  æ¨¡å‹: {model_info.get('provider', 'N/A')} / {model_info.get('model', 'N/A')}")

        # æ˜¾ç¤ºæç¤ºè¯
        print(f"\nğŸ“ å‘é€ç»™ LLM çš„æç¤ºè¯:")
        print("â”Œ" + "â”€" * 48 + "â”")
        # æˆªæ–­è¶…é•¿æç¤ºè¯
        if len(prompt) > 1000 and not self.verbose:
            truncated_prompt = prompt[:500] + "\n...\n[ä¸­é—´éƒ¨åˆ†å·²çœç•¥]...\n" + prompt[-500:]
            for line in truncated_prompt.split('\n'):
                print(f"â”‚ {line[:46]:<46} â”‚")
        else:
            for line in prompt.split('\n'):
                print(f"â”‚ {line[:46]:<46} â”‚")
        print("â””" + "â”€" * 48 + "â”˜")

        # æ˜¾ç¤ºå“åº”
        print(f"\nğŸ“¤ LLM åŸå§‹å“åº”:")
        print("â”Œ" + "â”€" * 48 + "â”")
        response_str = str(response)
        if hasattr(response, 'content'):
            response_str = response.content
        elif hasattr(response, 'text'):
            response_str = response.text
        elif isinstance(response, dict):
            response_str = json.dumps(response, ensure_ascii=False, indent=2)

        # æˆªæ–­è¶…é•¿å“åº”
        if len(response_str) > 800 and not self.verbose:
            truncated_response = response_str[:400] + "\n...\n[ä¸­é—´éƒ¨åˆ†å·²çœç•¥]...\n" + response_str[-400:]
            for line in truncated_response.split('\n'):
                print(f"â”‚ {line[:46]:<46} â”‚")
        else:
            for line in response_str.split('\n'):
                print(f"â”‚ {line[:46]:<46} â”‚")
        print("â””" + "â”€" * 48 + "â”˜")


class EnhancedSentimentAgent:
    """å¢å¼ºçš„æƒ…æ„Ÿåˆ†æ Agentï¼Œå¸¦æœ‰è¯¦ç»†è°ƒè¯•ä¿¡æ¯"""

    def __init__(self, original_agent, inspector: DetailedLLMInspector):
        self.original_agent = original_agent
        self.inspector = inspector
        self.agent_id = getattr(original_agent, 'agent_id', 'sentiment_analysis')
        self.name = "EnhancedSentimentAgent"

        # æ‹¦æˆªåŸå§‹çš„ invoke_llm æ–¹æ³•
        original_invoke_llm = getattr(self.original_agent, 'invoke_llm', None)
        if original_invoke_llm:
            self.original_agent.invoke_llm = self._wrap_invoke_llm(original_invoke_llm, "SentimentAgent")

    async def process_conversation(self, state):
        """åŒ…è£…åŸå§‹æ–¹æ³•ï¼Œæ·»åŠ è°ƒè¯•ä¿¡æ¯"""
        print(f"\nğŸ¯ ã€æƒ…æ„Ÿåˆ†æ Agentã€‘å¼€å§‹å¤„ç†...")

        # ä» state ä¸­æå–å®¢æˆ·è¾“å…¥
        customer_input = None
        if hasattr(state, 'input'):
            customer_input = state.input
        elif isinstance(state, dict):
            customer_input = state.get('input') or state.get('customer_input')

        print(f"   ğŸ“¥ æ¥æ”¶åˆ°çš„å®¢æˆ·è¾“å…¥: {customer_input}")

        # æ˜¾ç¤º state çš„è¯¦ç»†ä¿¡æ¯
        if isinstance(state, dict):
            print(f"   ğŸ“Š State ç±»å‹: dict, é”®: {list(state.keys())}")
        else:
            print(f"   ğŸ“Š State ç±»å‹: {type(state).__name__}")

        # è°ƒç”¨åŸå§‹å¤„ç†æ–¹æ³•
        result = await self.original_agent.process_conversation(state)

        print(f"   âœ… æƒ…æ„Ÿåˆ†æå®Œæˆ")

        # æ˜¾ç¤ºæƒ…æ„Ÿåˆ†æç»“æœ
        if isinstance(result, dict):
            sentiment_data = result.get('sentiment_analysis')
            intent_data = result.get('intent_analysis')

            if sentiment_data:
                print(f"   ğŸ’­ æƒ…æ„Ÿåˆ†æç»“æœ:")
                print(f"      â€¢ æƒ…æ„Ÿ: {sentiment_data.get('sentiment', 'N/A')}")
                print(f"      â€¢ åˆ†æ•°: {sentiment_data.get('score', 'N/A')}")
                print(f"      â€¢ æƒ…ç»ª: {sentiment_data.get('emotions', 'N/A')}")

            if intent_data:
                print(f"   ğŸ¯ æ„å›¾åˆ†æç»“æœ:")
                print(f"      â€¢ æ„å›¾: {intent_data.get('intent', 'N/A')}")
                print(f"      â€¢ ç±»åˆ«: {intent_data.get('category', 'N/A')}")

        return result

    def _wrap_invoke_llm(self, original_method, agent_name):
        """åŒ…è£… invoke_llm æ–¹æ³•ä»¥æ•è·è°ƒè¯•ä¿¡æ¯"""
        async def wrapper(request, *args, **kwargs):
            # æå–æç¤ºè¯ä¿¡æ¯
            prompt = "N/A"
            model_info = {
                "provider": request.provider if hasattr(request, 'provider') else 'Unknown',
                "model": request.model if hasattr(request, 'model') else 'Unknown'
            }

            if hasattr(request, 'messages') and request.messages:
                # æ„å»ºå®Œæ•´çš„æç¤ºè¯
                prompt_parts = []
                for msg in request.messages:
                    if hasattr(msg, 'role') and hasattr(msg, 'content'):
                        prompt_parts.append(f"[{msg.role}]: {msg.content}")
                    else:
                        prompt_parts.append(str(msg))
                prompt = "\n".join(prompt_parts)

            # è°ƒç”¨åŸå§‹æ–¹æ³•
            response = await original_method(request, *args, **kwargs)

            # æ˜¾ç¤ºè°ƒè¯•ä¿¡æ¯
            self.inspector.display_llm_call(agent_name, prompt, response, model_info)

            return response
        return wrapper


class EnhancedSalesAgent:
    """å¢å¼ºçš„é”€å”® Agentï¼Œå¸¦æœ‰è¯¦ç»†è°ƒè¯•ä¿¡æ¯"""

    def __init__(self, original_agent, inspector: DetailedLLMInspector):
        self.original_agent = original_agent
        self.inspector = inspector
        self.agent_id = getattr(original_agent, 'agent_id', 'sales_agent')
        self.name = "EnhancedSalesAgent"

        # æ‹¦æˆªåŸå§‹çš„ invoke_llm æ–¹æ³•ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
        original_invoke_llm = getattr(self.original_agent, 'invoke_llm', None)
        if original_invoke_llm:
            self.original_agent.invoke_llm = self._wrap_invoke_llm(original_invoke_llm, "SalesAgent")

        # æ‹¦æˆª response_service çš„ generate_response æ–¹æ³•
        if hasattr(self.original_agent, 'response_service'):
            original_generate_response = getattr(self.original_agent.response_service, 'generate_response', None)
            if original_generate_response:
                self.original_agent.response_service.generate_response = self._wrap_generate_response(original_generate_response, "SalesAgent")

    async def process_conversation(self, state):
        """åŒ…è£…åŸå§‹æ–¹æ³•ï¼Œæ·»åŠ è°ƒè¯•ä¿¡æ¯"""
        print(f"\nğŸ¯ ã€é”€å”® Agentã€‘å¼€å§‹å¤„ç†...")

        # æ˜¾ç¤ºæ¥æ”¶åˆ°çš„çŠ¶æ€ä¿¡æ¯
        print(f"   ğŸ“¥ æ¥æ”¶åˆ°çš„çŠ¶æ€ä¿¡æ¯:")

        # ä» state ä¸­æå–å®¢æˆ·è¾“å…¥
        customer_input = None
        if hasattr(state, 'input'):
            customer_input = state.input
        elif isinstance(state, dict):
            customer_input = state.get('input') or state.get('customer_input')

        print(f"   ğŸ“ ç”¨æˆ·è¾“å…¥: {customer_input}")

        # æ˜¾ç¤ºæ¥æ”¶åˆ°çš„æƒ…æ„Ÿåˆ†æç»“æœ
        sentiment_analysis = None
        intent_analysis = None

        if hasattr(state, 'sentiment_analysis'):
            sentiment_analysis = state.sentiment_analysis
        elif isinstance(state, dict):
            sentiment_analysis = state.get('sentiment_analysis')

        if hasattr(state, 'intent_analysis'):
            intent_analysis = state.intent_analysis
        elif isinstance(state, dict):
            intent_analysis = state.get('intent_analysis')

        if sentiment_analysis:
            print(f"      ğŸ’­ æƒ…æ„Ÿåˆ†æ:")
            print(f"         â€¢ æƒ…æ„Ÿ: {sentiment_analysis.get('sentiment', 'N/A')}")
            print(f"         â€¢ åˆ†æ•°: {sentiment_analysis.get('score', 'N/A')}")
            print(f"         â€¢ æƒ…ç»ª: {sentiment_analysis.get('emotions', 'N/A')}")

        if intent_analysis:
            print(f"      ğŸ¯ æ„å›¾åˆ†æ:")
            print(f"         â€¢ æ„å›¾: {intent_analysis.get('intent', 'N/A')}")
            print(f"         â€¢ ç±»åˆ«: {intent_analysis.get('category', 'N/A')}")

        # è°ƒç”¨åŸå§‹å¤„ç†æ–¹æ³•
        result = await self.original_agent.process_conversation(state)

        print(f"   âœ… é”€å”®å›å¤ç”Ÿæˆå®Œæˆ")

        # æ˜¾ç¤ºç”Ÿæˆçš„å›å¤
        if isinstance(result, dict):
            sales_response = result.get('sales_response') or result.get('output')
            if sales_response:
                print(f"   ğŸ“¤ ç”Ÿæˆçš„å›å¤: {sales_response[:100]}...")

        return result

    def _wrap_invoke_llm(self, original_method, agent_name):
        """åŒ…è£… invoke_llm æ–¹æ³•ä»¥æ•è·è°ƒè¯•ä¿¡æ¯"""
        async def wrapper(request, *args, **kwargs):
            # æå–æç¤ºè¯ä¿¡æ¯
            prompt = "N/A"
            model_info = {
                "provider": request.provider if hasattr(request, 'provider') else 'Unknown',
                "model": request.model if hasattr(request, 'model') else 'Unknown'
            }

            if hasattr(request, 'messages') and request.messages:
                # æ„å»ºå®Œæ•´çš„æç¤ºè¯
                prompt_parts = []
                for msg in request.messages:
                    if hasattr(msg, 'role') and hasattr(msg, 'content'):
                        prompt_parts.append(f"[{msg.role}]: {msg.content}")
                    else:
                        prompt_parts.append(str(msg))
                prompt = "\n".join(prompt_parts)

            # è°ƒç”¨åŸå§‹æ–¹æ³•
            response = await original_method(request, *args, **kwargs)

            # æ˜¾ç¤ºè°ƒè¯•ä¿¡æ¯
            self.inspector.display_llm_call(agent_name, prompt, response, model_info)

            return response
        return wrapper

    def _wrap_generate_response(self, original_method, agent_name):
        """åŒ…è£… response_service çš„ generate_response æ–¹æ³•ä»¥æ•è·è°ƒè¯•ä¿¡æ¯"""
        async def wrapper(customer_input, response_context, strategy="auto", *args, **kwargs):
            print(f"\n{'ğŸ” å“åº”ç”Ÿæˆè¯¦æƒ…' : ^50}")
            print("â”€" * 50)
            print(f"ğŸ¤– Agent: {agent_name}")
            print(f"ğŸ“ å®¢æˆ·è¾“å…¥: {customer_input}")
            print(f"ğŸ¯ ç­–ç•¥: {strategy}")

            # æ˜¾ç¤ºå“åº”ä¸Šä¸‹æ–‡
            print(f"\nğŸ“Š å“åº”ç”Ÿæˆä¸Šä¸‹æ–‡:")
            if isinstance(response_context, dict):
                for key, value in response_context.items():
                    if key in ['sentiment_analysis', 'intent_analysis']:
                        print(f"   â€¢ {key}: {json.dumps(value, ensure_ascii=False, indent=4)[:200]}...")
                    else:
                        print(f"   â€¢ {key}: {str(value)[:100]}...")

            # è°ƒç”¨åŸå§‹æ–¹æ³•
            response = await original_method(customer_input, response_context, strategy, *args, **kwargs)

            print(f"\nğŸ“¤ ç”Ÿæˆçš„æœ€ç»ˆå“åº”:")
            print(f"   ã€Œ{response[:200]}...ã€" if len(response) > 200 else f"   ã€Œ{response}ã€")

            return response
        return wrapper


async def test_with_detailed_real_agents(message: str | list, verbose: bool = False):
    """ä½¿ç”¨å¢å¼ºç‰ˆçœŸå®æ™ºèƒ½ä½“æµ‹è¯• (æ˜¾ç¤ºè¯¦ç»†è°ƒè¯•ä¿¡æ¯)

    å‚æ•°:
        message: ç”¨æˆ·è¾“å…¥ï¼Œå¯ä»¥æ˜¯å­—ç¬¦ä¸²æˆ–åˆ—è¡¨æ ¼å¼çš„å¤šæ¨¡æ€å†…å®¹
        verbose: æ˜¯å¦æ˜¾ç¤ºæè¯¦ç»†çš„è°ƒè¯•ä¿¡æ¯
    """
    print("=" * 80)
    print("  å¢å¼ºç‰ˆ Sentiment Chat Workflow æµ‹è¯• - è¯¦ç»†è°ƒè¯•æ¨¡å¼")
    print("=" * 80)

    try:
        # å¯¼å…¥å¿…è¦æ¨¡å—
        from core.workflows.sentiment_chat_workflow import SentimentChatWorkflow
        from core.app.entities import WorkflowExecutionModel
        from core.agents.sentiment.agent import SentimentAnalysisAgent
        from core.agents.sales.agent import SalesAgent
        from libs.constants import WorkflowConstants
        from langgraph.graph import StateGraph

        print("\nâœ… æ¨¡å—å¯¼å…¥æˆåŠŸ")

        # åˆ›å»ºè°ƒè¯•æ£€æŸ¥å™¨
        inspector = DetailedLLMInspector(verbose=verbose)

        # æ­¥éª¤ 1: åˆ›å»ºåŸå§‹æ™ºèƒ½ä½“
        print("\n" + "â”€" * 70)
        print("ğŸ“¦ æ­¥éª¤ 1: åˆ›å»ºå¢å¼ºç‰ˆæ™ºèƒ½ä½“ (å«è¯¦ç»†è°ƒè¯•)")
        print("â”€" * 70)

        original_sentiment_agent = SentimentAnalysisAgent()
        original_sales_agent = SalesAgent()

        # åˆ›å»ºå¢å¼ºç‰ˆæ™ºèƒ½ä½“
        enhanced_agents = {
            WorkflowConstants.SENTIMENT_NODE: EnhancedSentimentAgent(original_sentiment_agent, inspector),
            WorkflowConstants.SALES_NODE: EnhancedSalesAgent(original_sales_agent, inspector),
        }

        print(f"\nâœ… å·²åˆ›å»º {len(enhanced_agents)} ä¸ªå¢å¼ºç‰ˆæ™ºèƒ½ä½“:")
        for agent_id, agent in enhanced_agents.items():
            print(f"   - {agent_id}: {agent.__class__.__name__}")

        # æ­¥éª¤ 2: æ„å»ºå·¥ä½œæµ
        print("\n" + "â”€" * 70)
        print("ğŸ”§ æ­¥éª¤ 2: æ„å»ºå·¥ä½œæµå›¾")
        print("â”€" * 70)

        workflow = SentimentChatWorkflow(enhanced_agents)
        graph_builder = StateGraph(WorkflowExecutionModel)

        # æ³¨å†ŒèŠ‚ç‚¹
        workflow._register_nodes(graph_builder)
        workflow._define_edges(graph_builder)
        workflow._set_entry_exit_points(graph_builder)

        # ç¼–è¯‘å›¾
        graph = graph_builder.compile()

        print("âœ… å·¥ä½œæµå›¾æ„å»ºæˆåŠŸ")
        print(f"   - èŠ‚ç‚¹: {WorkflowConstants.SENTIMENT_NODE} â†’ {WorkflowConstants.SALES_NODE}")

        # æ­¥éª¤ 3: æ‰§è¡Œæµ‹è¯•
        print(f"\n{'=' * 80}")
        print(f"æµ‹è¯•æ¡ˆä¾‹ - è¯¦ç»†è°ƒè¯•æ¨¡å¼")
        print(f"{'=' * 80}")
        # æ˜¾ç¤ºç”¨æˆ·è¾“å…¥ä¿¡æ¯
        if isinstance(message, list):
            print(f"ğŸ’¬ ç”¨æˆ·è¾“å…¥ (å¤šæ¨¡æ€):")
            for i, item in enumerate(message, 1):
                item_type = item.get('type', 'unknown') if isinstance(item, dict) else 'unknown'
                item_content = item.get('content', str(item)) if isinstance(item, dict) else str(item)
                content_preview = item_content[:50] + "..." if len(item_content) > 50 else item_content
                print(f"   {i}. [{item_type}] {content_preview}")
        else:
            print(f"ğŸ’¬ ç”¨æˆ·è¾“å…¥: {message}")

        # åˆ›å»ºåˆå§‹çŠ¶æ€ - ç¡®ä¿inputå­—æ®µæ ¼å¼æ­£ç¡®
        initial_state = WorkflowExecutionModel(
            workflow_id=uuid4(),
            thread_id=uuid4(),
            tenant_id="test_tenant",
            assistant_id=uuid4(),
            input=message,
            timestamp="2024-01-01T00:00:00"
        )

        print(f"\nğŸš€ å¼€å§‹æ‰§è¡Œå·¥ä½œæµ...")
        if isinstance(message, list):
            print(f"   ğŸ“Š åˆå§‹çŠ¶æ€: å¤šæ¨¡æ€å†…å®¹ ({len(message)}é¡¹)")
        else:
            print(f"   ğŸ“Š åˆå§‹çŠ¶æ€: {message}")

        # æ‰§è¡Œå·¥ä½œæµå¹¶æ”¶é›†è¯¦ç»†ç»“æœ
        workflow_steps = []
        step_counter = 0

        async for step in graph.astream(initial_state):
            step_counter += 1
            workflow_steps.append(step)

            print(f"\n{'ğŸ”„ æ­¥éª¤ ' + str(step_counter) : ^60}")
            print("=" * 60)

            for node_name, node_output in step.items():
                print(f"\nğŸ“ èŠ‚ç‚¹ [{node_name}] æ‰§è¡Œç»“æœ:")
                print("â”€" * 50)

                # æ˜¾ç¤ºè¯¦ç»†çš„èŠ‚ç‚¹è¾“å‡ºåˆ†æ
                if isinstance(node_output, dict):
                    for key, value in node_output.items():
                        if key == 'input':
                            print(f"   ğŸ“¥ è¾“å…¥: {str(value)[:100]}...")
                        elif key == 'sentiment_analysis':
                            print(f"   ğŸ’­ æƒ…æ„Ÿåˆ†æ: {json.dumps(value, ensure_ascii=False, indent=6)}")
                        elif key == 'intent_analysis':
                            print(f"   ğŸ¯ æ„å›¾åˆ†æ: {json.dumps(value, ensure_ascii=False, indent=6)}")
                        elif key == 'sales_response':
                            print(f"   ğŸ¤– é”€å”®å›å¤: {value}")
                        elif key == 'output':
                            print(f"   ğŸ“¤ æœ€ç»ˆè¾“å‡º: {value}")
                        else:
                            print(f"   ğŸ”§ {key}: {str(value)[:100]}...")

        # è·å–æœ€ç»ˆç»“æœ
        final_result = {}
        for step in workflow_steps:
            final_result.update(step)

        # æ˜¾ç¤ºæœ€ç»ˆæ€»ç»“
        print(f"\n{'ğŸ“Š æœ€ç»ˆæ‰§è¡Œæ€»ç»“' : ^80}")
        print("=" * 80)
        print(f"âœ… å·¥ä½œæµæ‰§è¡ŒæˆåŠŸ")
        print(f"ğŸ“ˆ æ€»æ­¥éª¤æ•°: {step_counter}")
        print(f"ğŸ”§ æœ€ç»ˆçŠ¶æ€é”®: {list(final_result.keys())}")

        # æƒ…æ„Ÿåˆ†ææ€»ç»“
        sentiment_data = final_result.get('sentiment_analysis')
        if sentiment_data:
            print(f"\nğŸ’­ æƒ…æ„Ÿåˆ†ææœ€ç»ˆç»“æœ:")
            print(f"   â€¢ æƒ…æ„Ÿå€¾å‘: {sentiment_data.get('sentiment', 'N/A')}")
            print(f"   â€¢ æƒ…æ„Ÿåˆ†æ•°: {sentiment_data.get('score', 'N/A')}")
            print(f"   â€¢ æ£€æµ‹æƒ…ç»ª: {sentiment_data.get('emotions', 'N/A')}")
            print(f"   â€¢ æ»¡æ„åº¦: {sentiment_data.get('satisfaction', 'N/A')}")

        # æ„å›¾åˆ†ææ€»ç»“
        intent_data = final_result.get('intent_analysis')
        if intent_data:
            print(f"\nğŸ¯ æ„å›¾åˆ†ææœ€ç»ˆç»“æœ:")
            print(f"   â€¢ ä¸»è¦æ„å›¾: {intent_data.get('intent', 'N/A')}")
            print(f"   â€¢ æ„å›¾ç±»åˆ«: {intent_data.get('category', 'N/A')}")
            print(f"   â€¢ å†³ç­–é˜¶æ®µ: {intent_data.get('decision_stage', 'N/A')}")
            print(f"   â€¢ ç½®ä¿¡åº¦: {intent_data.get('confidence', 'N/A')}")

        # é”€å”®å›å¤æ€»ç»“
        sales_response = final_result.get('sales_response') or final_result.get('output')
        if sales_response:
            print(f"\nğŸ¯ æœ€ç»ˆ AI å›å¤:")
            print(f"   ã€Œ{sales_response}ã€")

        # LLM è°ƒç”¨å†å²æ€»ç»“
        if inspector.call_history:
            print(f"\nğŸ“ˆ LLM è°ƒç”¨ç»Ÿè®¡:")
            print(f"   â€¢ æ€»è°ƒç”¨æ¬¡æ•°: {len(inspector.call_history)}")
            for i, call in enumerate(inspector.call_history, 1):
                agent = call['agent_name']
                model = f"{call['model_info']['provider']}/{call['model_info']['model']}"
                print(f"   â€¢ è°ƒç”¨ {i}: {agent} â†’ {model}")

    except Exception as e:
        print(f"\nâŒ æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)


def create_multimodal_input(text_content: str, additional_items: list = None) -> list:
    """åˆ›å»ºå¤šæ¨¡æ€è¾“å…¥æ ¼å¼

    å‚æ•°:
        text_content: ä¸»è¦æ–‡æœ¬å†…å®¹
        additional_items: é¢å¤–çš„å¤šæ¨¡æ€é¡¹ç›®åˆ—è¡¨

    è¿”å›:
        list: å¤šæ¨¡æ€å†…å®¹åˆ—è¡¨
    """
    content_items = [
        {"type": "text", "content": text_content}
    ]

    if additional_items:
        content_items.extend(additional_items)

    return content_items


async def test_with_multimodal_example(verbose: bool = False):
    """ä½¿ç”¨å¤šæ¨¡æ€æ ¼å¼æµ‹è¯•çš„ç¤ºä¾‹"""
    # åˆ›å»ºå¤šæ¨¡æ€æµ‹è¯•è¾“å…¥
    multimodal_input = create_multimodal_input("æˆ‘æƒ³äº†è§£ä½ ä»¬çš„æœåŠ¡")

    print("=" * 80)
    print("  å¤šæ¨¡æ€æ ¼å¼æµ‹è¯•ç¤ºä¾‹")
    print("=" * 80)
    print(f"ğŸ¯ æµ‹è¯•è¾“å…¥æ ¼å¼: {json.dumps(multimodal_input, ensure_ascii=False, indent=2)}")

    await test_with_detailed_real_agents(multimodal_input, verbose)


async def batch_test_multiple_cases(verbose: bool = False):
    """æ‰¹é‡æµ‹è¯•å¤šä¸ªæ¡ˆä¾‹ï¼ˆåŒ…æ‹¬å¤šæ¨¡æ€æ ¼å¼ï¼‰"""
    # æµ‹è¯•æ¡ˆä¾‹ - åŒ…å«å­—ç¬¦ä¸²å’Œå¤šæ¨¡æ€æ ¼å¼
    test_cases = [
        "ä½ å¥½",  # ä¼ ç»Ÿå­—ç¬¦ä¸²æ ¼å¼
        create_multimodal_input("æˆ‘æƒ³äº†è§£ç¾ç™½é¡¹ç›®çš„ä»·æ ¼"),  # å¤šæ¨¡æ€æ ¼å¼
        "ä½ ä»¬çš„æœåŠ¡æ€ä¹ˆæ ·ï¼Ÿ",
        create_multimodal_input("æˆ‘å¯¹ä½ ä»¬çš„äº§å“ä¸å¤ªæ»¡æ„"),  # å¤šæ¨¡æ€æ ¼å¼
        "èƒ½ç»™æˆ‘æ¨èä¸€äº›é€‚åˆæ•æ„Ÿè‚Œçš„äº§å“å—ï¼Ÿ",
        # å¤æ‚å¤šæ¨¡æ€ç¤ºä¾‹ï¼ˆå¦‚æœéœ€è¦çš„è¯ï¼‰
        create_multimodal_input(
            "æˆ‘å¯¹è¿™ä¸ªäº§å“å¾ˆæ„Ÿå…´è¶£",
            [{"type": "image_url", "content": "https://example.com/product.jpg"}]
        )
    ]

    print("=" * 80)
    print("  æ‰¹é‡æµ‹è¯•æ¨¡å¼ - å¤šä¸ªæµ‹è¯•æ¡ˆä¾‹")
    print("=" * 80)

    for i, message in enumerate(test_cases, 1):
        print(f"\n{'ğŸ”„ æµ‹è¯•æ¡ˆä¾‹ ' + str(i) + ' / ' + str(len(test_cases)) : ^80}")
        await test_with_detailed_real_agents(message, verbose)

        if i < len(test_cases):
            print(f"\n{'â±ï¸  ç­‰å¾… 3 ç§’åè¿›è¡Œä¸‹ä¸€ä¸ªæµ‹è¯•...' : ^80}")
            await asyncio.sleep(3)


async def main():
    """ä¸»æµ‹è¯•æµç¨‹"""
    parser = argparse.ArgumentParser(description="å¢å¼ºç‰ˆ Sentiment Chat Workflow æµ‹è¯•å·¥å…·")
    parser.add_argument(
        "--real-llm",
        action="store_true",
        help="ä½¿ç”¨çœŸå® LLM (éœ€è¦é…ç½® API keys)"
    )
    parser.add_argument(
        "--message",
        type=str,
        default="ä½ å¥½",
        help="æŒ‡å®šæµ‹è¯•æ¶ˆæ¯"
    )
    parser.add_argument(
        "--verbose",
        action="store_true",
        help="æ˜¾ç¤ºæè¯¦ç»†çš„è°ƒè¯•ä¿¡æ¯ï¼ˆåŒ…æ‹¬å®Œæ•´æç¤ºè¯å’Œå“åº”ï¼‰"
    )
    parser.add_argument(
        "--batch",
        action="store_true",
        help="æ‰¹é‡æµ‹è¯•å¤šä¸ªé¢„è®¾æ¡ˆä¾‹"
    )
    parser.add_argument(
        "--multimodal",
        action="store_true",
        help="ä½¿ç”¨å¤šæ¨¡æ€æ ¼å¼æµ‹è¯•æ‚¨çš„åŸå§‹è¯·æ±‚"
    )

    args = parser.parse_args()

    if not args.real_llm:
        print("âš ï¸  æ­¤å¢å¼ºç‰ˆæµ‹è¯•è„šæœ¬ä»…æ”¯æŒçœŸå® LLM æ¨¡å¼")
        print("    è¯·ä½¿ç”¨ --real-llm å‚æ•°")
        sys.exit(1)

    if args.batch:
        await batch_test_multiple_cases(args.verbose)
    elif args.multimodal:
        await test_with_multimodal_example(args.verbose)
    else:
        await test_with_detailed_real_agents(args.message, args.verbose)


if __name__ == "__main__":
    print("""
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘                                                                  â•‘
â•‘     å¢å¼ºç‰ˆ Sentiment Chat Workflow - è¯¦ç»†è°ƒè¯•æµ‹è¯•è„šæœ¬ v1.0        â•‘
â•‘                                                                  â•‘
â•‘  æ–°å¢åŠŸèƒ½:                                                        â•‘
â•‘    â€¢ æ˜¾ç¤ºå®Œæ•´çš„ LLM æç¤ºè¯å’Œå“åº”                                  â•‘
â•‘    â€¢ æ˜¾ç¤ºå„ä¸ª Agent çš„è¯¦ç»†å¤„ç†æ­¥éª¤                                â•‘
â•‘    â€¢ æ”¯æŒæ‰¹é‡æµ‹è¯•å¤šä¸ªæ¡ˆä¾‹                                         â•‘
â•‘    â€¢ å¯é€‰çš„è¶…è¯¦ç»†è°ƒè¯•æ¨¡å¼                                         â•‘
â•‘                                                                  â•‘
â•‘  ä½¿ç”¨ç¤ºä¾‹:                                                        â•‘
â•‘    python test_sentiment_detailed.py --real-llm --message "ä½ å¥½" â•‘
â•‘    python test_sentiment_detailed.py --real-llm --multimodal     â•‘
â•‘    python test_sentiment_detailed.py --real-llm --batch          â•‘
â•‘    python test_sentiment_detailed.py --real-llm --verbose        â•‘
â•‘                                                                  â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
""")

    asyncio.run(main())