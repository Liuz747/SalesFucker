"""
Sales Agent - ç®€åŒ–ç‰ˆï¼ˆä½¿ç”¨æ™ºèƒ½åŒ¹é…æç¤ºè¯ + è®°å¿†ç³»ç»Ÿï¼‰

åŸºäº SentimentAgent è¾“å‡ºçš„ matched_promptï¼Œç»“åˆè®°å¿†ä¸Šä¸‹æ–‡ç”Ÿæˆä¸ªæ€§åŒ–é”€å”®å›å¤ã€‚
ç§»é™¤å¤æ‚çš„äº§å“æ¨èé€»è¾‘ï¼Œä¸“æ³¨äºæ ¸å¿ƒå¯¹è¯ç”Ÿæˆã€‚

æ ¸å¿ƒèŒè´£:
- æ¥æ”¶ matched_promptï¼ˆæƒ…æ„Ÿé©±åŠ¨çš„æç¤ºè¯ï¼‰
- é›†æˆè®°å¿†ä¸Šä¸‹æ–‡
- ç”Ÿæˆä¸ªæ€§åŒ–é”€å”®å›å¤
- è®°å¿†å­˜å‚¨ç”±å·¥ä½œæµå±‚çº§ç»Ÿä¸€å¤„ç†
"""

from typing import Dict, Any, Tuple
from uuid import uuid4

from ..base import BaseAgent
from libs.types import Message
from infra.runtimes.entities import CompletionsRequest
from utils import get_current_datetime
from config import mas_config


class SalesAgent(BaseAgent):
    """
    é”€å”®æ™ºèƒ½ä½“ - ç®€åŒ–ç‰ˆ

    è®¾è®¡ç†å¿µï¼š
    - ä½¿ç”¨ SentimentAgent åŒ¹é…çš„æç¤ºè¯ï¼Œè€Œä¸æ˜¯é‡æ–°ç”Ÿæˆ
    - é›†æˆè®°å¿†ç³»ç»Ÿæä¾›ä¸Šä¸‹æ–‡è¿è´¯æ€§
    - æç®€æ¶æ„ï¼šæ¥æ”¶â†’å¤„ç†â†’ç”Ÿæˆï¼Œè®°å¿†å­˜å‚¨ç”±å·¥ä½œæµç»Ÿä¸€å¤„ç†
    """

    def __init__(self):
        super().__init__()

        # ç§»é™¤ç‹¬ç«‹çš„ StorageManagerï¼Œè®°å¿†ç®¡ç†ç”±å·¥ä½œæµå±‚çº§å¤„ç†
        self.llm_provider = mas_config.DEFAULT_LLM_PROVIDER
        self.llm_model = "openai/gpt-5-mini"

    async def process_conversation(self, state: dict) -> dict:
        """
        å¤„ç†å¯¹è¯çŠ¶æ€ï¼ˆç®€åŒ–ç‰ˆï¼šä½¿ç”¨åŒ¹é…æç¤ºè¯ + è®°å¿†ä¸Šä¸‹æ–‡ï¼‰

        å·¥ä½œæµç¨‹ï¼š
        1. è¯»å– SentimentAgent è¾“å‡ºçš„ matched_prompt å’Œ memory_context
        2. æ„å»ºå¢å¼ºçš„ LLM æç¤ºè¯ï¼ˆåŒ…å«å†å²è®°å¿†ï¼‰
        3. ç”Ÿæˆä¸ªæ€§åŒ–é”€å”®å›å¤
        4. è®°å¿†å­˜å‚¨ç”±å·¥ä½œæµå±‚çº§ç»Ÿä¸€å¤„ç†

        å‚æ•°:
            state: åŒ…å« matched_prompt, memory_context, customer_input ç­‰

        è¿”å›:
            dict: æ›´æ–°åçš„å¯¹è¯çŠ¶æ€ï¼ŒåŒ…å« sales_response
        """
        start_time = get_current_datetime()

        try:
            self.logger.info("=== Sales Agent å¼€å§‹å¤„ç† ===")

            # è¯»å– SentimentAgent ä¼ é€’çš„æ•°æ®
            customer_input = state.get("customer_input", "")
            matched_prompt = state.get("matched_prompt", {})
            memory_context = state.get("memory_context", {})

            self.logger.info(f"æ¥æ”¶æ•°æ® - è¾“å…¥é•¿åº¦: {len(customer_input)}, åŒ¹é…æç¤ºè¯: {matched_prompt.get('matched_key', 'unknown')}")
            self.logger.info(f"è®°å¿†ä¸Šä¸‹æ–‡ - çŸ­æœŸ: {len(memory_context.get('short_term', []))} æ¡, é•¿æœŸ: {len(memory_context.get('long_term', []))} æ¡")

            # ç”Ÿæˆä¸ªæ€§åŒ–å›å¤ï¼ˆåŸºäºåŒ¹é…çš„æç¤ºè¯ + è®°å¿†ï¼‰
            sales_response, token_info = await self._generate_response_with_memory(
                customer_input, matched_prompt, memory_context
            )

            # æ›´æ–°çŠ¶æ€
            updated_state = self._update_state(state, sales_response, token_info)

            processing_time = (get_current_datetime() - start_time).total_seconds()
            self.logger.info(f"é”€å”®å›å¤ç”Ÿæˆå®Œæˆ: è€—æ—¶{processing_time:.2f}s, é•¿åº¦={len(sales_response)}, tokens={token_info.get('tokens_used', 0)}")
            self.logger.info("=== Sales Agent å¤„ç†å®Œæˆï¼ˆç®€åŒ–ç‰ˆï¼‰ ===")

            return updated_state

        except Exception as e:
            self.logger.error(f"é”€å”®ä»£ç†å¤„ç†å¤±è´¥: {e}", exc_info=True)
            return self._create_error_state(state, str(e))

    async def _generate_response_with_memory(
        self, customer_input: str, matched_prompt: Dict[str, Any], memory_context: Dict[str, Any]
    ) -> Tuple[str, Dict[str, Any]]:
        """
        ğŸ”¥ æ–°å¢ï¼šåŸºäºåŒ¹é…æç¤ºè¯å’Œè®°å¿†ç”Ÿæˆå›å¤

        Args:
            customer_input: å®¢æˆ·è¾“å…¥
            matched_prompt: SentimentAgent åŒ¹é…çš„æç¤ºè¯
            memory_context: è®°å¿†ä¸Šä¸‹æ–‡

        Returns:
            tuple: (å›å¤å†…å®¹, tokenä¿¡æ¯)
        """
        try:
            # 1. æ„å»ºåŸºç¡€ system promptï¼ˆæ¥è‡ªåŒ¹é…å™¨ï¼‰
            system_prompt = matched_prompt.get("system_prompt", "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„ç¾å®¹é¡¾é—®ã€‚")
            tone = matched_prompt.get("tone", "ä¸“ä¸šã€å‹å¥½")
            strategy = matched_prompt.get("strategy", "æ ‡å‡†æœåŠ¡")

            # 2. æ·»åŠ è®°å¿†ä¸Šä¸‹æ–‡
            memory_text = self._format_memory_context(memory_context)

            # 3. æ„å»ºå¢å¼ºçš„ç³»ç»Ÿæç¤º
            enhanced_system_prompt = f"""
            {system_prompt}

            ã€è¯­æ°”è¦æ±‚ã€‘{tone}
            ã€ç­–ç•¥è¦æ±‚ã€‘{strategy}

            {memory_text}

            ã€å›å¤è¦æ±‚ã€‘
            - ç”¨ä¸­æ–‡å›å¤ï¼Œè¯­è¨€è‡ªç„¶æµç•…
            - æ§åˆ¶åœ¨150å­—ä»¥å†…
            - ä½“ç°ä¸ªæ€§åŒ–ï¼Œé¿å…æ¨¡æ¿åŒ–å›å¤
            - æ ¹æ®å®¢æˆ·å†å²é€‚åº¦è°ƒæ•´ç­–ç•¥
            """

            # 4. æ„å»ºå¯¹è¯æ¶ˆæ¯
            messages = [
                {"role": "system", "content": enhanced_system_prompt.strip()},
                {"role": "user", "content": customer_input}
            ]

            # 5. è°ƒç”¨ LLM
            request = CompletionsRequest(
                id=uuid4(),
                provider=self.llm_provider,
                model=self.llm_model,
                temperature=0.7,  # é€‚åº¦åˆ›é€ æ€§
                messages=[Message(role=msg["role"], content=msg["content"]) for msg in messages]
            )

            llm_response = await self.invoke_llm(request)

            # 6. æå– token ä¿¡æ¯
            token_info = self._extract_token_info(llm_response)

            # 7. è¿”å›å“åº”
            if llm_response and llm_response.content:
                response_content = str(llm_response.content).strip()
                self.logger.debug(f"LLM å›å¤é¢„è§ˆ: {response_content[:100]}...")
                return response_content, token_info
            else:
                return self._get_fallback_response(matched_prompt), {}

        except Exception as e:
            self.logger.error(f"å›å¤ç”Ÿæˆå¤±è´¥: {e}")
            return self._get_fallback_response(matched_prompt), {"tokens_used": 0, "error": str(e)}

    def _format_memory_context(self, memory_context: dict) -> str:
        """
        æ ¼å¼åŒ–è®°å¿†ä¸Šä¸‹æ–‡ä¸º LLM å¯ç”¨çš„æ–‡æœ¬

        Args:
            memory_context: è®°å¿†ä¸Šä¸‹æ–‡å­—å…¸

        Returns:
            str: æ ¼å¼åŒ–åçš„è®°å¿†æ–‡æœ¬
        """
        parts = []

        # é•¿æœŸè®°å¿†æ‘˜è¦
        long_term = memory_context.get("long_term", [])
        if long_term:
            summaries = []
            for memory in long_term[:3]:  # æœ€å¤š 3 æ¡æ‘˜è¦
                content = memory.get("content", "")
                if content:
                    summaries.append(f"- {content[:100]}")  # é™åˆ¶é•¿åº¦

            if summaries:
                parts.append("ã€å®¢æˆ·å†å²èƒŒæ™¯ã€‘\n" + "\n".join(summaries))

        # çŸ­æœŸå¯¹è¯å†å²
        short_term = memory_context.get("short_term", [])
        if short_term and len(short_term) > 2:  # æœ‰è¶³å¤Ÿçš„å¯¹è¯å†å²
            recent_exchanges = []
            for msg in short_term[-4:]:  # æœ€è¿‘ 4 æ¡æ¶ˆæ¯
                role = msg.get("role", "")
                content = str(msg.get("content", ""))[:80]  # é™åˆ¶é•¿åº¦
                if role == "user":
                    recent_exchanges.append(f"å®¢æˆ·: {content}")
                elif role == "assistant":
                    recent_exchanges.append(f"æˆ‘: {content}")

            if recent_exchanges:
                parts.append("ã€æœ€è¿‘å¯¹è¯ã€‘\n" + "\n".join(recent_exchanges))

        # å¦‚æœæ²¡æœ‰è®°å¿†ï¼Œæ·»åŠ é¦–æ¬¡å¯¹è¯æç¤º
        if not parts:
            parts.append("ã€å®¢æˆ·ä¿¡æ¯ã€‘è¿™æ˜¯ä¸è¯¥å®¢æˆ·çš„é¦–æ¬¡å¯¹è¯ã€‚")

        return "\n\n".join(parts)

    def _extract_token_info(self, llm_response) -> dict:
        """æå– token ä½¿ç”¨ä¿¡æ¯"""
        try:
            if llm_response and hasattr(llm_response, 'usage') and isinstance(llm_response.usage, dict):
                usage = llm_response.usage
                return {
                    "tokens_used": usage.get("input_tokens", 0) + usage.get("output_tokens", 0),
                    "input_tokens": usage.get("input_tokens", 0),
                    "output_tokens": usage.get("output_tokens", 0)
                }
        except Exception as e:
            self.logger.warning(f"Token ä¿¡æ¯æå–å¤±è´¥: {e}")

        return {"tokens_used": 0}

    def _get_fallback_response(self, matched_prompt: dict) -> str:
        """è·å–å…œåº•å›å¤"""
        tone = matched_prompt.get("tone", "ä¸“ä¸šã€å‹å¥½")

        if "æ¸©å’Œ" in tone or "å…³æ€€" in tone:
            return "æˆ‘ç†è§£æ‚¨çš„æ„Ÿå—ï¼Œä½œä¸ºæ‚¨çš„ç¾å®¹é¡¾é—®ï¼Œæˆ‘ä¼šè€å¿ƒä¸ºæ‚¨æä¾›ä¸“ä¸šå»ºè®®ã€‚è¯·å‘Šè¯‰æˆ‘æ‚¨é‡åˆ°çš„å…·ä½“é—®é¢˜ã€‚"
        elif "ç§¯æ" in tone or "çƒ­æƒ…" in tone:
            return "å¤ªå¥½äº†ï¼æˆ‘æ˜¯æ‚¨çš„ä¸“ä¸šç¾å®¹é¡¾é—®ï¼Œå¾ˆé«˜å…´ä¸ºæ‚¨æœåŠ¡ï¼è¯·å‘Šè¯‰æˆ‘æ‚¨çš„ç¾å®¹éœ€æ±‚ï¼Œæˆ‘ä¼šä¸ºæ‚¨æä¾›æœ€é€‚åˆçš„å»ºè®®ã€‚"
        else:
            return "æ„Ÿè°¢æ‚¨çš„å’¨è¯¢ã€‚æˆ‘æ˜¯æ‚¨çš„ä¸“ä¸šç¾å®¹é¡¾é—®ï¼Œå¾ˆä¹æ„ä¸ºæ‚¨æä¾›ä¸ªæ€§åŒ–çš„äº§å“å»ºè®®å’Œç¾å®¹æ–¹æ¡ˆã€‚"

    def _update_state(self, state: dict, sales_response: str, token_info: dict) -> dict:
        """æ›´æ–°å¯¹è¯çŠ¶æ€"""
        # ä¸»è¦çŠ¶æ€ï¼ˆLangGraph ä¼ é€’ï¼‰
        state["sales_response"] = sales_response
        state["output"] = sales_response  # ä½œä¸ºæœ€ç»ˆè¾“å‡º

        # å¤‡ä»½åˆ° values ç»“æ„
        if state.get("values") is None:
            state["values"] = {}
        if state["values"].get("agent_responses") is None:
            state["values"]["agent_responses"] = {}

        state["values"]["agent_responses"][self.agent_id] = {
            "sales_response": sales_response,
            "tokens_used": token_info.get("tokens_used", 0),
            "timestamp": get_current_datetime(),
            "response_length": len(sales_response)
        }

        # æ›´æ–°æ´»è·ƒä»£ç†åˆ—è¡¨
        state.setdefault("active_agents", []).append(self.agent_id)

        self.logger.info(f"çŠ¶æ€æ›´æ–°å®Œæˆ - æœ€ç»ˆè¾“å‡ºé•¿åº¦: {len(sales_response)}")
        return state


    def health_check(self) -> dict:
        """å¥åº·æ£€æŸ¥"""
        try:
            # æµ‹è¯•åŸºæœ¬åŠŸèƒ½
            test_prompt = {
                "system_prompt": "ä½ æ˜¯æµ‹è¯•é¡¾é—®",
                "tone": "å‹å¥½",
                "strategy": "æµ‹è¯•"
            }
            test_memory = {"short_term": [], "long_term": []}

            # æ¨¡æ‹Ÿç”Ÿæˆå›å¤ï¼ˆé€šè¿‡ fallbackï¼‰
            response = self._get_fallback_response(test_prompt)

            return {
                "status": "healthy",
                "llm_provider": self.llm_provider,
                "llm_model": self.llm_model,
                "memory_manager": "workflow_level",  # æ›´æ–°ä¸ºå·¥ä½œæµçº§åˆ«
                "test_response_length": len(response)
            }
        except Exception as e:
            return {
                "status": "unhealthy",
                "error": str(e)
            }
